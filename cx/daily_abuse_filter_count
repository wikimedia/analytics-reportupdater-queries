#!/bin/bash
timestamp=$1
year=`echo $timestamp | cut -b1-4`
month=`echo $timestamp | cut -b6-7`
day=`echo $timestamp | cut -b9-10`

hive -e "

CREATE EXTERNAL TABLE IF NOT EXISTS \`amire80.cx_abuse_filter_daily\` (
  \`project\`         string COMMENT 'wiki db name',
  \`abuse_filter_id\` bigint COMMENT 'AbuseFilter ID in that wiki',
  \`trigger_count\`   bigint COMMENT 'Value of the metric.'
)
PARTITIONED BY (
  \`year\`            int    COMMENT 'Unpadded year',
  \`month\`           int    COMMENT 'Unpadded month',
  \`day\`             int    COMMENT 'Unpadded day'
)
STORED AS PARQUET
LOCATION '/user/amire80/data/cx_abuse_filter_daily'
;

-- Get the daily counts without any personal
-- or article data and insert it into the storage table
insert into table amire80.cx_abuse_filter_daily
  partition(
    year=$year,
    month=$month,
    day=$day
  )
select
  wiki as project,
  abuse_filter_id,
  count(wiki) as trigger_count
from (
  select distinct
    concat(
      event.filterId,
      event.sourceLanguage,
      event.sourceTitle,
      event.targetLanguage,
      event.token
    ) as session,
    event.filterId as abuse_filter_id,
    wiki
  from
    event.ContentTranslationAbuseFilter
  where
    year = CAST($year AS int) and
    month = CAST($month AS int) and
    day = CAST($day AS int)
) as source
group by
  abuse_filter_id,
  wiki;

-- As we're using RU to populate another table,
-- instead of updating a report, we have to trick it to make
-- it think that it is generating a report.
-- See https://phabricator.wikimedia.org/T189475#4770686
select
  date('$timestamp') as \`date\`,
  true as done

" 2> /dev/null | grep -v parquet.hadoop | grep -v 'project' # removes insert select headers
